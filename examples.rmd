---
title: "Parallel computing in R. A brief introduction and a few examples"
author:
    - names: "Sergio Picó Jordá"
    - affiliations: INMAR, Departamento de Biología, Universidad de Cádiz.
date: "`r Sys.Date()`"
output:
    distill::distill_article:
        highlight: kate
        code_folding: false
        toc: true
        toc_depth: 3
        css: styles.css
editor_options:
    chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```


# Parallel computing in R. A brief introduction and a few examples.



## What is parallel computing?
Parallel computation is the simultaneous executions of different pieces of a
(normally large) computation across multiple processors or cores. The interest of
this technique is that you can execute a process that takes *x* seconds on a
single processor in *x/n* seconds using *n* processors (this is the ideal case,
that is rarely possible to achieve).

We can use this in a supercomputer (like the [UCA's
cluster](https://supercomputacion.uca.es/)), but also in almost any device with multiple processors or cores. And happily nowadays most of our PCs and laptops have multicore processors.

How serial computing sometimes looks like in our
computers, as it happens most of the time in R (awesome GIF from my WOT gaming years):

![CPU 0 doing all the heavy lifting](https://tenor.com/view/wot-cpu-danceing-break-dancing-cool-gif-16563810.gif)

Instead, we want our cores to be a team!:

![Teamwork!](https://media.giphy.com/media/qowPpT1lFY89MN2BVn/giphy.gif)



## How to do parallel computing in R

Ok, so how does this work? We need to create a structure that sets a *director node* that is going to distribute data and functions between the *workers*, that will execute the interactions. This structure will allocate and manage the resorces, like number of cores, RAM memory and so. I will show you two different systems: **FORK** and
**PSOCK**.

## Examples

### Making apply functions parallel

We can make the functions from the *apply* family parallel , [example by Jens Moll-Elsborg](https://towardsdatascience.com/getting-started-with-parallel-programming-in-r-d5f801d43745). Create the data:

```{create data}
# Create a vector with 1 billion elements
data <- 1:1e9

# Make a list of 4 of these vectors
data_list <- list("1" = data,
                  "2" = data,
                  "3" = data,
                  "4" = data
)
```
Calculate the mean 

```{serial mean}
# Calculate the mean of every vector
time_benchmark <- system.time(
    lapply(data_list, mean)
)
time_benchmark # Total of 12.83s in my machine
```
This is fine, but let's make our cores share the effort:

Linux/Mac version:

```{parallel mean linux/mac}
# Load package parallel
library(parallel)
detectCores()

parallel::mclapply(1:10, function(x) {
                   mean(data_list)
          },       mc.cores = 8)
```

Windows version:

```{parallel mean windows}
# Load package parallel
library(parallel)

# Detect number of cores, create a local cluster
parallel::detectCores()
cl <- parallel::makeCluster(8)

# Run the same process but in parallel
time_parallel  <- system.time(
    parallel::parLapply(cl,
                        data_list,
                        mean)
)

# Stop cluster
parallel::stopCluster(cl)
```

### Running a loop in parallel

We can also parallelise a for loop.

Linux/Mac version:

```{loop parallel Linux/Mac}
# Load packages
library(doParallel)
library(parallel)
library(foreach)

# Activate the cluster for foreach
doParallel::registerDoParallel(cores = 8)

# Calculate mean
time_foreach <- system.time({
     r <- foreach(i = 1:length(data_list),
                  .combine = rbind) %dopar% {
          mean(data_list[[i]])
     }
}
)

time_foreach[3]
```

Windows version:

```{loop parallel windows}
library(doParallel)
library(parallel)
library(foreach)

# Detect number of cores and create cluster
cl <- parallel::makeCluster(detectCores())

# Activate the cluster for foreach
doParallel::registerDoParallel(cl)

# Calculate mean
time_foreach <- system.time({
    r <- foreach(i = 1:length(data_list),
                 .combine = rbind) %dopar% {
         mean(data_list[[i]])
    }
}
)

time_foreach[3]

# Stop cluster
parallel::stopCluster(cl)
```


### A real example: Random forest with *ranger*

This is a great example showed by Blas Benito in [his blog](https://www.blasbenito.com/post/02_parallelizing_loops_with_r/).

```{load packages}
library(palmerpenguins)
library(ranger)
```

The goal of this process is to classify penguins from different species using measurements of bill length, bill depth, flipper length, and body mass. Prepare the data:

```{prepare data}
penguins <- as.data.frame(
    na.omit(
        penguins[, c("species",
                     "bill_length_mm",
                     "bill_depth_mm",
                     "flipper_length_mm",
                     "body_mass_g"
        )]
    )
)
```

Now, fit a random forest model using the *ranger* package:

```{fit rf}

# Fit model
m <- ranger::ranger(
    data = penguins,
    dependent.variable.name = "species",
    importance = "permutation"
)

# Check results
m
m$variable.importance

```

Now, let's do a grid search for hyperparameter optimization. We are going to fit many models with diferent hyperparameter combinations to find the combination with the best prediction error. Hyperparameters: number of trees (num.trees), number of variables selected by chance for a tree split (mtry), mminimum number of cases in a terminal node (min.node.size).

```{optimization}

# Create a table with all combinations
sensitivity.df  <- expand.grid(
    num.trees = c(500, 1000, 1500),
    mtry = 2:4,
    min.node.size = c(1, 10, 20)
)

# 27 combinations of hyperparameters

# Create and register cluster
cl <- parallel:makeCluster(8)
doParallel::registerDoParallel(cl)

# Fit models
prediction.error <- foreach(
# Iterate over the hyperparameter data frame
    num.trees = sensitivity.df$num.trees,
    mtry = sensitivity.df$mtry,
    min.node.size = sensitivity.df$min.node.size,
    .combine = "c",
    . packages = "ranger"
) %dopar% {

    # Fit model
    m.i <- ranger::ranger(
        data = penguins,
        dependent.variable.name = "species",
        num.trees = num.trees,
        mtry = mtry,
        min.node.size = min.node.size
    )
return(m.i$prediction.error * 100)
}

# Add the prediction error column to the data frame
sensitivity.df$prediction.error <- prediction.error

# Stop the cluster
parallel::stopCluster(cl)
```

Let's plot the results:

```{plot sensitivity}
ggplot2::ggplot(data = sensitivity.df) +
    ggplot2::aes(
        x = mtry,
        y = as.factor(min.node.size),
        fill = prediction.error
    ) +
    ggplot2::facet_wrap(as.factor(sensitivity.df$num.trees)) +
    ggplot2::geom_tile() +
    ggplot2::scale_y_discrete(breaks = c(1, 10, 20)) +
    ggplot2::scale_fill_viridis_c() +
    ggplot2::ylab("min.node.size")
```

Now let's find the combination of hyperparameters with the lowest prediction error:

```{best hyper}
best.hyperparameters <- sensitivity.df %>%
    dplyr::arrange(prediction.error) %>%
    dplyr::slice(1)
```



### Parallel computing in the tidyverse: *future* and *furrr*

If you like tidyverse and use *map()*, you may have thought about how good would be to use that family of functions in parallel. [*Furrr*](https://furrr.futureverse.org/) combines the [*purrr*](https://purrr.tidyverse.org/reference/map.html) family of mapping functions with [*futures*](https://future.futureverse.org/)'s parallel processing capabilities. For example, we can replace the original *map()* for *future_map()* to map in parallel!

Do you remember our first example? We could have done it using map too:

```{map}
time_map <- system.time(
    purrr::map(data_list, mean)
)
time_map
time_benchmark
```

It took the same time than using lapply, because it is being executed serially. Let's make map parallel:

```{furrr}
# Check the number of cores available
future::availableCores()

# Cluster for Windows
plan(cluster, workers = 4)

# Cluster for Linux and Mac
plan(multicore, workers = 4)

# Calculate the mean in parallel
time_future <- system.time(
    furrr::future_map(data_list, mean)
)
time_future
```
We reduced the processing time to one third, it works!

### Final considerations

When we compute things in parallel we have to be aware of how much RAM each process will need. We may have to lower the number of cores used.

Be careful with what object you decide to return from each iteration. If you return a whole model each time, the object where you are storing your results could grow very fast.

Also, the communication between workers can be a limitation. If big files have to be transfered many times, it could offset the process time we reduced by parallelization.